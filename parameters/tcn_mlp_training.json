{
    "n_epochs" : 100,
    "batch_size": 32 ,
    "obs_length": 8,
    "pred_length"  : 12,
    "seq_length"  : 20,
    "nb_samples" : 9892,
    "num_workers" : 0,
    "lr" : 0.001, 
    "load_path" : "",    
    "plot": 1,
    "input_dim":2,
    "mlp_layers":[500,500,250,175],
    "output_size":2,
    "model_type": 0,
    "plot_every": 5,
    "save_every":5,
    "kernel_size":2,
    "dropout" : 0.2,
    "nb_conv_feat":32,
    "offsets":1

}